{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edc8312",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0953c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Maja Kubara\"\n",
    "STUDENT_ID = \"14498863\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669fd616",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf49909-80cf-409a-b44d-8553e16effc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b585cf74037b4c712eb518f3ef8e765",
     "grade": false,
     "grade_id": "cell-3cff1dd74792b356",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Analyzing Gender Distribution Among Scientific Authors in Computational Social Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620fa176-4f89-4b75-96ba-634fc9b8962a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee897c4f558978be6819e4f4f7c261f5",
     "grade": false,
     "grade_id": "cell-8c3327acf8f9bd6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Objective*: Understand the gender distribution of authors across different scientific disciplines using web scraping and API-based gender identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39693a-b0d4-4372-a0e0-ecca61ba40c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c16f3e433bf6dac45bbe3f424b3b397",
     "grade": false,
     "grade_id": "cell-e817d3557fb4f0df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Gender diversity in research is crucial for ensuring diverse perspectives and approaches in scientific inquiry, and for the comprehensiveness and richness of research findings. A balanced gender representation can help challenge systemic biases that might otherwise marginalize or overlook significant areas of study. A diverse research community can also act as a role model, inspiring future generations of all genders to pursue scientific endeavors.\n",
    "\n",
    "This assignment focuses on the question of the gender distribution of researchers in different disciplines, and on identifying how often women are the first or last author of publications. \n",
    "\n",
    "To do so, you will scrape a preprint website, and you will use the API genderize.io to identify the gender of the author based on their name.\n",
    "\n",
    "1. Prepare: Identify a source and decide a scraping strategy\n",
    "\n",
    "2. Scrape the list of articles and authors\n",
    "\n",
    "3. Use API to identify gender \n",
    "\n",
    "4. Analyze gender distribution and authorship order\n",
    "\n",
    "5. Reflect on your findings. \n",
    "\n",
    "6. Scrape the paper abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f155b-74cb-427d-8650-9b76782acb00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1dda8234fd20fa3e245cee7db24992a9",
     "grade": false,
     "grade_id": "cell-df1834b4ac2ae69b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Setup and requirements\n",
    "First make sure that you have the needed libraries for Python correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6ecfdd-54b5-49ab-b84b-279c4831b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium\n",
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# !pip install webdriver-manager --upgrade\n",
    "# !pip install packaging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b9c67f-2327-4373-9a39-f7b94360b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request\n",
    "#!pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb1d394-4f99-49d2-a25c-edf6af82561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautifulsoup\n",
    "#!pip install beautifulsoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6550b7-edfc-47df-af69-3d9665090102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12a9d5-8975-425f-8e13-dc4e586090ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30d4be4fd67982bfddf22fbe5c9fdf48",
     "grade": false,
     "grade_id": "cell-dcc2e17cee32d989",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Plan and strategize\n",
    "\n",
    "We first need to decide which site to scrape and our strategy for doing so. We will focus on a preprint repository. Preprint repositories host and disseminate research papers before they are peer-reviewed and published in academic journals. They therefore give a view of the latest research.\n",
    "\n",
    "There are several repositories that represent different scientific disciplines (e.g., PubMed for life sciences, arXiv for physics and computer science, JSTOR for humanities and social sciences, SocArxiv for social science, etc.) \n",
    "\n",
    "We will here focus on arxiv.org, where many Computational Social Scientists publish, often under the category \"Computers and Society\".\n",
    "\n",
    "You need to pick a page on ArXiv where you can get a representative sample of these research papers -- and which you are allowed to scrape.\n",
    "\n",
    "1. Browse Arxiv.org, and select a page on the website where you can find a sample of research papers.\n",
    "2. Check the robots.txt. Are you allowed to scrape the page you selected? (If not, you will have to choose another one!)\n",
    "3. Decide a strategy for scraping the page as quickly and easily as possible to find the names of the authors for each paper, their titles, and a link to the pages.\n",
    "4. Choose which Python libraries for scraping that you will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723d96b-af05-42a4-981b-4d8d1722e22d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6b6e1409e86d810ef169596cbabc039",
     "grade": false,
     "grade_id": "cell-4696e716d4f1c81a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1: Which library is most suitable?\n",
    "\n",
    "Given the structure of the website, which Python libraries for scraping do you think is appropriate to use? Motivate your choice in a few sentences.\n",
    "\n",
    "_Json parsing as it is easier to scrape and then turn into a dictionary _\n",
    "\n",
    "[Evaluation: This is an open question. Any motivation that makes sense is fine, but in general, requests make more sense for this page than selenium, since the site in question is not dynamic. Using selenium will be slower and more difficult.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4455efa-4750-4473-aa08-db19ba9bafce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "594b965b0d990ca90076303a53a26d39",
     "grade": false,
     "grade_id": "cell-f043dfd5a37ede8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Scrape the list of articles and authors \n",
    "\n",
    "Implement your scraping strategy. Scrape the page and collect the information about the publication. \n",
    "\n",
    "- You will need to get (1) the link to the article, (2) the title of the article, (3) the names of all authors of the paper, in the same order as they appear on the paper. \n",
    "- You need to scrape 200 research papers.\n",
    "\n",
    "- Note that you may need to iterate over multiple pages.\n",
    "- Note that you need to handle possible exceptions and that your code needs to be able to restart if it crashes.\n",
    "- You final result should be a list of dicts, with keys 'title', 'url', and 'authors'. 'authors' should consist of a list where the authors are listed in the order that they were on the paper. \n",
    "- You need to clean and validate your data: check that all papers have authors, that all papers have titles, clean the texts to remove empty spaces and similar, etc.\n",
    "- Store the resulting array persistently as a pickle with the name 'scraping_result.pkl'.\n",
    "\n",
    "For instance: [{'title': 'How to use Large Langauge Models for Text Analysis', 'authors': ['TÃ¶rnberg, Petter'], 'url':'https://arxiv.org/abs/2307.13106' } ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9449314",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53252983d2c99f0b67a82089ef4b22fb",
     "grade": false,
     "grade_id": "cell-f23671663c59f383",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import urllib\n",
    "import feedparser\n",
    "\n",
    "# Access url, read it and parse\n",
    "base_url = 'http://export.arxiv.org/api/query?'\n",
    "query =  'search_query=cat:stat.ME&start=0&max_results=200'\n",
    "url = urllib.request.urlopen(base_url + query)\n",
    "response = url.read()\n",
    "data = feedparser.parse(response)\n",
    "\n",
    "# Iterate through each entry in the data, save url, title, and authors of each article\n",
    "article_info = []\n",
    "for article in data.entries:\n",
    "    url = article.link\n",
    "    title = article.title\n",
    "    authors = article.authors\n",
    "    # Save it in a dictionary and append in a list\n",
    "    info = {'title':title, 'authors':authors, 'url':url}\n",
    "    article_info.append(info)\n",
    "\n",
    "# Saving entries that have authors\n",
    "article_info = [article for article in article_info if article['authors']]\n",
    "\n",
    "# Removing extra spaces\n",
    "data_list = []\n",
    "for entry in article_info:\n",
    "    data_dict = {key: value.strip() if isinstance(value, str) else value for key, value in entry.items()}\n",
    "    data_list.append(data_dict)\n",
    "\n",
    "#store the results as a pickle\n",
    "with open ('scraping_result.pkl', 'wb') as file:\n",
    "    pickle.dump(data_list, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13393063",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a24a8ecb5c00b7a230f1536e1032db2e",
     "grade": true,
     "grade_id": "cell-eb1c8710173e94df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if keys exists in dictionary\n",
    "assert 'title' in data_list[0], \"Key 'title' not found in dictionary\"\n",
    "assert 'authors' in data_list[0], \"Key 'author' not found in dictionary\"\n",
    "assert 'url' in data_list[0], \"Key 'url' not found in dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac684d5-9ff9-48f2-81bf-1aa4e18ade2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb97b7c9455088e95da62d87f93f0237",
     "grade": true,
     "grade_id": "cell-a20385a62f48c168",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d989cacd-dff0-456e-8967-2dbe08362fc0",
   "metadata": {},
   "source": [
    "## 3. Use Genderize.io to identify author gender\n",
    "\n",
    "The next step is to identify the gender of the authors. To do so, we will use the free API genderdize.io. \n",
    "\n",
    "1. Go to https://genderize.io/ and read the API documnentation.\n",
    "2. Do you need to register to use it? Do you need an API key? \n",
    "3. How do you call the API? What parameters do you need to send? \n",
    "4. What rate limiting is used? How long do you need to wait between calls?\n",
    "\n",
    "You will use what you learned to carry out the following tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065292a-42e8-4069-8dfd-4d6a1db353fa",
   "metadata": {},
   "source": [
    "#### Task 1: _identify_gender()_\n",
    "Write a function _identify_gender(first_name)_ that takes a name, and uses the API to guess the gender. The function should send a request to genderize.io, and parse the resulting json to a dict. The function should return a dict with the data provided by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d01c681e-6bce-416d-9c45-03810e9c2376",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8699ffe64f521fe51e152a3c4baae18",
     "grade": false,
     "grade_id": "cell-c02e3ed6f4cf8078",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 14512, 'name': 'Sasha', 'gender': 'female', 'probability': 0.52}\n"
     ]
    }
   ],
   "source": [
    "# Import gender API and save as a function\n",
    "def identify_gender(first_name):\n",
    "    \n",
    "    url = f'https://api.genderize.io?name={first_name}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Test\n",
    "print(identify_gender(\"Sasha\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7dc7f-c47f-4fb1-85db-096a46d5d5c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05b69c0a6698acd0a17d3faae33a79f8",
     "grade": true,
     "grade_id": "cell-ed3a813912da7ef2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03ef969-2f1e-4ebf-bd1f-3dd0657f2e69",
   "metadata": {},
   "source": [
    "#### Task 2: Identify gender of all authors\n",
    "\n",
    "Your task is now to use your new function to identify the genders of all authors that you previously scraped. \n",
    "\n",
    "To do so, you first need to extract the first name of each author. You need to iterate over these names and use your function to identify the gender of the author.\n",
    "\n",
    "Your result should be a dataframe with the following columns:\n",
    "\n",
    "- article_url | author_full_name | first_name | author_order | estimated_gender | gender_probability\n",
    "\n",
    "Author_order should be a number specifying where the author was in the author list for the publication (e.g., 0 = first author, 1 = second author...) _Estimated_gender_ should contain the API response on gender, and _gender_probability_ the certainty of the gender, according to the API.\n",
    "\n",
    "Note:\n",
    "- You will need to transform your dict to the dataframe shown above, with one author per line. (This means that each URL will be associated to multiple author names.)\n",
    "- Make sure that you respect the rate limiting of the API. \n",
    "- Make sure that you handle exceptions and that your function continues \n",
    "- Note that you get a maximum of 1,000 free calls per day, so you need to make sure that you do not waste your API calls!\n",
    "- The API may not have all names stored. For these names, store a _np.nan_ value as the gender.\n",
    "\n",
    "Pickle the resulting dataframe with the name: 'author_gender.df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db8df5b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a2558664a48ddbac7c92c35c0c14e2b",
     "grade": false,
     "grade_id": "cell-b9581c1efe1807cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Load scraped data\n",
    "with open('scraping_result.pkl', 'rb') as f:\n",
    "    data_list = pickle.load(f)\n",
    "\n",
    "articles_info = []\n",
    "\n",
    "# Iterate over each article in a list\n",
    "for article in data_list: \n",
    "    # Save url, title and authors of each article, count the order of autors  \n",
    "    url = article['url']\n",
    "    title = article['title']\n",
    "    authors = article.get(\"authors\", [])\n",
    "    order = 0\n",
    "\n",
    "    # Iterate over each author in a list\n",
    "    for author in authors: \n",
    "        # Get name of each author\n",
    "        full_name = author.get(\"name\")\n",
    "        full_name = str(full_name)\n",
    "\n",
    "        # Check if the author value is missing\n",
    "        if full_name: \n",
    "            # Get first name      \n",
    "            full_name = full_name.split()\n",
    "            name = full_name[0]\n",
    "            # Use gender function\n",
    "            gender_info = identify_gender(name)\n",
    "            # Save gender and probability\n",
    "            gender = gender_info.get('gender')\n",
    "            probability = gender_info.get('probability')\n",
    "\n",
    "        #if missing then np.nan   \n",
    "        else:               \n",
    "            name = [np.nan]\n",
    "\n",
    "        #create a dictionary and append to a list   \n",
    "        article_info = {'article_url':url, 'author_full_name':full_name, 'first_name':name,'author_order':order,'estimated_gender':gender,'gender_probability':probability}\n",
    "        articles_info.append(article_info)\n",
    "\n",
    "        order += 1\n",
    "        \n",
    "   \n",
    "\n",
    "df = pd.DataFrame(articles_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b002fc50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94b2428a7e2b93e5c2e81bca2122b796",
     "grade": true,
     "grade_id": "cell-85d226433e71226b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>author_full_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>author_order</th>\n",
       "      <th>estimated_gender</th>\n",
       "      <th>gender_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/0705.0700v3</td>\n",
       "      <td>[Raydonal, Ospina]</td>\n",
       "      <td>Raydonal</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/0705.0700v3</td>\n",
       "      <td>[Silvia, L., P., Ferrari]</td>\n",
       "      <td>Silvia</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/0705.2938v1</td>\n",
       "      <td>[Guilhem, Coq]</td>\n",
       "      <td>Guilhem</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/0705.2938v1</td>\n",
       "      <td>[Olivier, Alata]</td>\n",
       "      <td>Olivier</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/0705.2938v1</td>\n",
       "      <td>[Marc, Arnaudon]</td>\n",
       "      <td>Marc</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://arxiv.org/abs/0705.2938v1</td>\n",
       "      <td>[Christian, Olivier]</td>\n",
       "      <td>Christian</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://arxiv.org/abs/0705.4588v1</td>\n",
       "      <td>[Shurong, Zheng]</td>\n",
       "      <td>Shurong</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://arxiv.org/abs/0705.4588v1</td>\n",
       "      <td>[Guodong, Song]</td>\n",
       "      <td>Guodong</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://arxiv.org/abs/0705.4588v1</td>\n",
       "      <td>[Ning-Zhong, Shi]</td>\n",
       "      <td>Ning-Zhong</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://arxiv.org/abs/0706.1287v1</td>\n",
       "      <td>[Helen, Armstrong]</td>\n",
       "      <td>Helen</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        article_url           author_full_name  first_name  \\\n",
       "0  http://arxiv.org/abs/0705.0700v3         [Raydonal, Ospina]    Raydonal   \n",
       "1  http://arxiv.org/abs/0705.0700v3  [Silvia, L., P., Ferrari]      Silvia   \n",
       "2  http://arxiv.org/abs/0705.2938v1             [Guilhem, Coq]     Guilhem   \n",
       "3  http://arxiv.org/abs/0705.2938v1           [Olivier, Alata]     Olivier   \n",
       "4  http://arxiv.org/abs/0705.2938v1           [Marc, Arnaudon]        Marc   \n",
       "5  http://arxiv.org/abs/0705.2938v1       [Christian, Olivier]   Christian   \n",
       "6  http://arxiv.org/abs/0705.4588v1           [Shurong, Zheng]     Shurong   \n",
       "7  http://arxiv.org/abs/0705.4588v1            [Guodong, Song]     Guodong   \n",
       "8  http://arxiv.org/abs/0705.4588v1          [Ning-Zhong, Shi]  Ning-Zhong   \n",
       "9  http://arxiv.org/abs/0706.1287v1         [Helen, Armstrong]       Helen   \n",
       "\n",
       "   author_order estimated_gender  gender_probability  \n",
       "0             0             None                0.00  \n",
       "1             1           female                1.00  \n",
       "2             0             male                0.99  \n",
       "3             1             male                1.00  \n",
       "4             2             male                1.00  \n",
       "5             3             male                1.00  \n",
       "6             0           female                0.64  \n",
       "7             1             male                0.96  \n",
       "8             2             None                0.00  \n",
       "9             0           female                1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert 'article_url' in df.columns, \"article_url column is missing\"\n",
    "assert 'author_full_name' in df.columns, \"author_full_name column is missing\"\n",
    "assert 'first_name' in df.columns, \"first_name column is missing\"\n",
    "assert 'author_order' in df.columns, \"author_order column is missing\"\n",
    "assert 'estimated_gender' in df.columns, \"estimated_gender column is missing\"\n",
    "assert 'gender_probability' in df.columns, \"gender_probability column is missing\"\n",
    "\n",
    "with open('author_gender.df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04701544-19ce-4f09-b6fc-06d1cae74f58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f43b38f6f15f75ac827995ace4fe885",
     "grade": true,
     "grade_id": "cell-c2e95f57fe249bfe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efaadaf-c7cf-4287-88a4-27b2bdf846e4",
   "metadata": {},
   "source": [
    "## 4. Analyze gender distribution and authorship order\n",
    "\n",
    "Now that you have gathered the necessary data, you will use this data to answer some research questions about gender equality in CSS research. Note that in calculating this, you need to handle that the API may have failed to identify the gender of some authors.\n",
    "\n",
    "1. What fraction of the authors included are women? \n",
    "2. What happens to this fraction if you only include authors for which the gender_probability is higher than 80%? \n",
    "3. Being the first or single author on a research paper is an important status signal among researchers: it often means that you made the most work. What fraction of the first or single authors are women? \n",
    "4. Being the _last_ author on a research paper with _three or more authors_ is also an important status signal: it tends to mean that you were the one to acquire funding or lead the lab. What fraction of the last-authors on papers with three or more author are women?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cbbcb4f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e46a8a2f7e1cabdb0061c0d4c9fc86f2",
     "grade": true,
     "grade_id": "cell-3a0cd012a322701a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24 of authors are women\n",
      "If the probability is changed to above 80% the fraction equals to 0.18\n",
      "The amount of women that are either first or a single author is 0.1\n",
      "0.22877358490566038\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "criteria_1 = df['estimated_gender'] == \"female\"\n",
    "female_total = criteria_1.sum()\n",
    "total_rows = len(df)\n",
    "female_fraction = female_total/total_rows\n",
    "female_fraction = round(female_fraction, 2)\n",
    "print(f'{female_fraction} of authors are women')\n",
    "#Question 2\n",
    "criteria_2 = (df['estimated_gender'] == \"female\") & (df['gender_probability'] > 0.8)\n",
    "female_total_2 = criteria_2.sum()\n",
    "fraction_2 = female_total_2/total_rows\n",
    "fraction_2 = round(fraction_2, 2)\n",
    "print(f'If the probability is changed to above 80% the fraction equals to {fraction_2}')\n",
    "#Question 3\n",
    "criteria_3 = (df['author_order'] == 0) & (df['estimated_gender'] == \"female\")\n",
    "single_female = criteria_3.sum()\n",
    "fraction_3 = single_female/total_rows\n",
    "fraction_3 = round(fraction_3, 2)\n",
    "print(f'The amount of women that are either first or a single author is {fraction_3}')\n",
    "#Question 4\n",
    "criteria_4= df[(df['author_order'] <=3) &(df['estimated_gender']=='female')]\n",
    "female= len(criteria_4)\n",
    "fraction_4 = (female)/(total_rows)\n",
    "print(f'{fraction_4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b370f3-4b58-41f0-b615-5aff1c4a3ab6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66aa822667d179fcd1119f84e5100ee3",
     "grade": false,
     "grade_id": "cell-906708bcf0e226e3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## 5. Reflect on your findings\n",
    "\n",
    "You have now carried out your analysis of the gender distribution in articles in CSS using scraped data. Reflect on your findings and method, and answer each of the following questions in a few sentences.\n",
    "\n",
    "1. What are the implications of the observed gender distribution and author order in CSS? How do these distributions compare with your expectations?\n",
    "2. How accurate do you think your findings are? What are the limitations of determining gender based solely on names? Are there cultural or regional nuances that the API might miss?\n",
    "3. Reflect on the ethical considerations involved in scraping this data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27044038-f459-45c7-9a7f-d2cd6d15b18d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "253f673902e8de295d8442ad59caa4b8",
     "grade": true,
     "grade_id": "cell-47e469c422287e72",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. The fraction of women is less than fraction of men authors, more male authors are first in order\n",
    "2. Not accurate, the names could be used for both genders depepdning on a religion, culture, orvpersonal preferences\n",
    "3. Consent, the authors did not have a chance to consent to use their data for study and Transparency, authors are not aware of the study purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d395f6-7186-4239-93db-e3aa75f6abfb",
   "metadata": {},
   "source": [
    "## 6. Scrape the paper abstract\n",
    "\n",
    "Your next task is to get the abstract for each paper. You will use these abstracts in a later exercise in the course, where we will use text analysis to examine whether the content of research papers are a function of the gender of the author. \n",
    "\n",
    "To do so, you need to iterate over the papers that you have already identified, and scrape the abstract from the URL listed. \n",
    "\n",
    "#### Task 1: scrape_abstract()\n",
    "Write a function scrape_abstract(url) that goes to the research paper URL, and scrapes the content of the abstract. The function should return the abstract as a string, and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a98b5f1-ffd3-4150-a60c-2e738c884956",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea05833922353eeb9ca3fd40d86801b7",
     "grade": false,
     "grade_id": "cell-b8712361e6327b18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Abstract: This guide introduces Large Language Models (LLM) as a highly versatile text analysis method within the social sciences. As LLMs are easy-to-use, cheap, fast, and applicable on a broad range of text analysis tasks, ranging from text annotation and classification to sentiment analysis and critical discourse analysis, many scholars believe that LLMs will transform how we do text analysis. This how-to guide is aimed at students and researchers with limited programming experience, and offers a simple introduction to how LLMs can be used for text analysis in your own research project, as well as advice on best practices. We will go through each of the steps of analyzing textual data with LLMs using Python: installing the software, setting up the API, loading the data, developing an analysis prompt, analyzing the text, and validating the results. As an illustrative example, we will use the challenging task of identifying populism in political texts, and show how LLMs move beyond the existing state-of-the-art.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_abstract(url):\n",
    "    # get url and parse html\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    #find abstract by class name and extract it\n",
    "    abstract = soup.find('blockquote',class_=\"abstract mathjax\")\n",
    "    abstract_text = abstract.get_text(separator = ' ')\n",
    "    \n",
    "    return abstract_text\n",
    "\n",
    "\n",
    "# Test\n",
    "url = \"https://arxiv.org/abs/2307.13106\"\n",
    "print(scrape_abstract(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4466e2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "284598e8243153347beb6d3b0f5691f7",
     "grade": true,
     "grade_id": "cell-862386d3cf70edf2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4783d9a5-b38d-4ccc-9cbe-f2269467e6d8",
   "metadata": {},
   "source": [
    "#### Task 2: Scrape all urls\n",
    "\n",
    "You will now use your function to scrape all the URLs that you collected in step 2.\n",
    "\n",
    "The following will provide instructions for how you can go about this task. However, there are several ways to do this, and you are free to choose your preferred method.\n",
    "\n",
    "Prepare your data:\n",
    "\n",
    "1. Load your list of dicts from step 2 (scraping_result.pkl)\n",
    "2. Use it to create a dataframe. \n",
    "3. Add a column 'scraped' which should be False for all rows, and a column 'abstract' that should be None for all rows.\n",
    "4. Store the dataframe persistently (e.g., by pickling it.)\n",
    "\n",
    "The scraping procedure:\n",
    "\n",
    "1. Load the persitent pickle as dataframe (so that if your computer crashes, the function will continue where you were)\n",
    "2. Repeat the following steps until there are no more rows with scraped == False:\n",
    "3. Fetch a random row with scraped == False\n",
    "4. Go to the URL and scrape the abstract.\n",
    "5. Set abstract column in the dataframe to the resulting abstract, set scraped to True.\n",
    "6. Store the dataframe persistently as a pickle. \n",
    "\n",
    "Remember: \n",
    "- You may use another strategy. However, since you will be scraping many pages, you should expect your scraper to encounter problems along the way. You therefore need to make sure that you regularly store the results persistently.\n",
    "- Make sure to handle any exceptions gracefully.\n",
    "- Be respectful toward the website owners: wait at least one second between each call. \n",
    "\n",
    "Your final result should be a dataframe stored as 'scraped_abstracts.df.pkl', with filled 'abstract' and 'scraped' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbacc7-e0a1-4f98-8234-0ef5f76f3489",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<!-- [Evaluation: ]\n",
    "- Load dataframe as df\n",
    "- Check that the len of df = len of the result list from question 2. \n",
    "- Check that each line has an abstract, with len() > 100 e.g.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee8f157-0b13-4a9b-b4ff-56c738754500",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc342ab1d66dd404139011d72ebc4627",
     "grade": false,
     "grade_id": "cell-ac850524f08140c8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>scraped</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inflated Beta Distributions</td>\n",
       "      <td>[{'name': 'Raydonal Ospina'}, {'name': 'Silvia...</td>\n",
       "      <td>http://arxiv.org/abs/0705.0700v3</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n Abstract:   A sequential multiple testing p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codage arithmetique pour la description d'une ...</td>\n",
       "      <td>[{'name': 'Guilhem Coq'}, {'name': 'Olivier Al...</td>\n",
       "      <td>http://arxiv.org/abs/0705.2938v1</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n Abstract:   A sequential multiple testing p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Variable Selection Incorporating Prior Constra...</td>\n",
       "      <td>[{'name': 'Shurong Zheng'}, {'name': 'Guodong ...</td>\n",
       "      <td>http://arxiv.org/abs/0705.4588v1</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n Abstract:   A sequential multiple testing p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayesian Covariance Matrix Estimation using a ...</td>\n",
       "      <td>[{'name': 'Helen Armstrong'}, {'name': 'Christ...</td>\n",
       "      <td>http://arxiv.org/abs/0706.1287v1</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n Abstract:   A sequential multiple testing p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sensitivity of principal Hessian direction ana...</td>\n",
       "      <td>[{'name': 'Luke A. Prendergast'}, {'name': 'Jo...</td>\n",
       "      <td>http://arxiv.org/abs/0706.1408v1</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n Abstract:   A sequential multiple testing p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                        Inflated Beta Distributions   \n",
       "1  Codage arithmetique pour la description d'une ...   \n",
       "2  Variable Selection Incorporating Prior Constra...   \n",
       "3  Bayesian Covariance Matrix Estimation using a ...   \n",
       "4  Sensitivity of principal Hessian direction ana...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [{'name': 'Raydonal Ospina'}, {'name': 'Silvia...   \n",
       "1  [{'name': 'Guilhem Coq'}, {'name': 'Olivier Al...   \n",
       "2  [{'name': 'Shurong Zheng'}, {'name': 'Guodong ...   \n",
       "3  [{'name': 'Helen Armstrong'}, {'name': 'Christ...   \n",
       "4  [{'name': 'Luke A. Prendergast'}, {'name': 'Jo...   \n",
       "\n",
       "                                url scraped  \\\n",
       "0  http://arxiv.org/abs/0705.0700v3    True   \n",
       "1  http://arxiv.org/abs/0705.2938v1    True   \n",
       "2  http://arxiv.org/abs/0705.4588v1    True   \n",
       "3  http://arxiv.org/abs/0706.1287v1    True   \n",
       "4  http://arxiv.org/abs/0706.1408v1    True   \n",
       "\n",
       "                                            abstract  \n",
       "0  \\n Abstract:   A sequential multiple testing p...  \n",
       "1  \\n Abstract:   A sequential multiple testing p...  \n",
       "2  \\n Abstract:   A sequential multiple testing p...  \n",
       "3  \\n Abstract:   A sequential multiple testing p...  \n",
       "4  \\n Abstract:   A sequential multiple testing p...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#open pickle file\n",
    "with open ('scraping_result.pkl', 'rb') as file:\n",
    "    data_list = pickle.load(file)\n",
    "\n",
    "#convert to dataframe\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "#add scraped and abstract columns \n",
    "df['scraped'] = 'False'\n",
    "df['abstract'] = 'None'\n",
    "\n",
    "#for each link scrape abstract and save it in the dataframe\n",
    "for link in df['url']:\n",
    "    abstract = scrape_abstract(link)\n",
    "    if abstract:\n",
    "        df['scraped'] = 'True'\n",
    "        df['abstract'] = abstract\n",
    "    \n",
    "display(df.head(5))\n",
    "\n",
    "# Rename and save the final dataframe\n",
    "df.to_pickle('scraped_abstracts.df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95ec0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd9348c76775a31d0bc7c64630beda0c",
     "grade": true,
     "grade_id": "cell-d2927458cb5edcd5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
